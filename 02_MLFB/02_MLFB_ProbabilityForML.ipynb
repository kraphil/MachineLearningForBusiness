{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_MLFB_ProbabilityForML.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Probability for ML - Teil I</h1>\n",
        "\n",
        "<u>Agenda:</u>\n",
        "\n",
        "\n",
        "1.   Background - Probability im Kontext von ML\n",
        "2.   Anwendungsbeispiele\n",
        "3.   Ressourcen zum Lernen\n",
        "4.   Zusammenfassung\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y47hOEpxBF9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**1. Background - Probability im Kontext von ML**"
      ],
      "metadata": {
        "id": "3Opxf2o7DWwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Was versteht man unter Wahrscheinlichkeit / Wahrscheinlichkeitstheorie?"
      ],
      "metadata": {
        "id": "4ijJwEUFEM7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Die Wahrscheinlichkeitstheorie ist ein Feld der Mathematik, die uns eine Sprache zur Verfügung stellt, um Unsicherheiten in Bezug auf ein Event zu quantifizieren.\n",
        "\n",
        "\n",
        "\n",
        "*   Sicherheiten sind eher die Ausnahme, darum ist es besser mit Unsicherheiten zu arbeiten\n",
        "*   Wahrscheinlichkeit (Probability) quantifiziert den Glauben (Likelihood) daran, dass ein Ereignis eintritt.\n",
        "*   Wahrscheinlichkeitstheorie ist die Mathematik der Unsicherheiten\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S_SdMpbdEaAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Was ist der Unterschied zwischen Wahrscheinlichkeitstheorie und Statistik?"
      ],
      "metadata": {
        "id": "n-66O-UjnJIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Wahrscheinlichkeit: Die Wahrscheinlichkeitsrechnung (Probability) befasst sich mit der Vorhersage der Eintrittswahrscheinlichkeit (Likelihood) zukünftiger Ereignisse.\n",
        "*   Statistik: Statistik umfasst die Analyse der zeitlichen Häufigkeit von Ereignissen in der Vergangenheit.\n",
        "\n"
      ],
      "metadata": {
        "id": "nAMV7O_fUB-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wo liegt der Unterschied zwischen Probability und Likelihood?"
      ],
      "metadata": {
        "id": "1GBI4a04VmSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eine adäquate deutsche Übersetzung, ist schwer zu finden (DeepL: Probabilty, Likelihood - Wahrscheinlichkeit, Wahscheinlichkeit)\n",
        "\n",
        "**Beispiel**: Wenn ich eine faire Münze habe, dann ist die \"Probability\", dass sie Kopf ergibt bei 0,5. Wenn ich eine Münze 100-mal werfe und sie 52-mal Kopf ergibt, dann ist der/das \"Likelihood\" hoch, dass sie fair ist. \n",
        "\n",
        "Einfach: Likelihood liefert support für unsere angenomme Probability.\n",
        "\n"
      ],
      "metadata": {
        "id": "o37Ng9GaVsaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probability und Machine Learning"
      ],
      "metadata": {
        "id": "54qe6DQJpdKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Unsicherheiten machen die Entwicklung von ML-Modellen deutlich schwieriger.\n",
        "*   Ein Rauschen in den Daten, eine unvollständige Abdeckung der Domaine und nicht-perfekte Modelle stellen die drei größten Quellen für Unsicherheiten im maschinellen Lernen dar.\n",
        "*   Eines der größten Benefits probabilistischer Modelle ist, dass sie uns eine Eindruck über die Unsicherheit vermitteln, die mit einer Vorhersage verbunden ist.  \n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Wichtig:** Um Machine Learning als ein Werkzeug zur Problemlösung einzusetzen, muss man kein Wahrscheinlichkeitstheoretiker sein! Die grundlegenden Konzepte verstanden zu haben hilft allerdings die Funktions- und Wirkungsweise eines ML-Algortihmus nochvollziehen und diese weiterentwickeln zu können.\n",
        "\n"
      ],
      "metadata": {
        "id": "G-syruaSqORi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**2. Anwendungsbeispiele**"
      ],
      "metadata": {
        "id": "dcZwPSV5DQZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 Anwendungsbeispiel 1**"
      ],
      "metadata": {
        "id": "W5TpEjo84Jz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Bayesian Probability***"
      ],
      "metadata": {
        "id": "08uRq8oFMvfI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Das Bayes Theorem ist eine Methode, um bedingte Wahrschinlichkeiten zu bestimmen\n",
        "*   Die bedingte Wahrscheinlichkeit ist die Wahrscheinlichkeit eines Ereignisses in Abhängigkeit vom Eintreten eines anderen Ereignisses.\n",
        "\n",
        "![grafik.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "SKlUI6qaNFAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Naive Bayes Classifier**"
      ],
      "metadata": {
        "id": "jf4zyb6i4lZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Bayes Classifier: Probabilistisches Modell, das die wahrscheinlichste Vorhersage für neue Beispiele trifft.\n",
        "*   Versucht die Frage zu beantworten: *Was ist die wahrscheinlichste Klassifizierung der neuen Instanz angesichts der bekannten Trainingsdaten?*\n",
        "*   **Naive** Bayes Classifier: Stellt die Annahme voraus, dass die Variablen der Eingabedaten unabhängig voneinander sind.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "Arten von Naive Bayes Classifiern:\n",
        "\n",
        "\n",
        "1.   Binomialverteilung (für binäre Daten): Binomial Naive Bayes\n",
        "2.   Multinomialverteilung (für kategoriale Werte = Text): Multinomial Naive Bayes\n",
        "3.   Gaußsche Verteilung (für numerische Werte = Zahlen): Gaussian Naive Bayes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kr4UDiA3LaTI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beispiel Naive Bayes Classifier - Der ausführliche Weg"
      ],
      "metadata": {
        "id": "nAMDhBDJKqYq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Schritt 1: Wir erzeugen einen Datensatz bestehend aus 2 numerischen Features und Klassenlabel in der Form 0 oder 1."
      ],
      "metadata": {
        "id": "X44qU3Z4pQim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import von make_blobs aus der sklearn Bibliothek zur Generierung eines Datensatzes\n",
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "#2D Datensatz erzeugen mit 100 Beispielen\n",
        "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
        "\n",
        "#Zusammenfassung\n",
        "print(X.shape, y.shape)\n",
        "print(X[:5])\n",
        "print(y[:5])"
      ],
      "metadata": {
        "id": "MTZzM36BKHhg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Schritt 2: Bei dem generierten Datensatz handelt es sich um numerische Features. Das heißt, wir nutzen eine gaußsche Wahrscheinlichkeitsverteilung.\n",
        "Dafür werden zunächst Parameter wie *Durschnitt* und *Standardabweichung* des DS bestimmt. Daraus lässt sich eine Datenverteilung bestimmen. Am Ende müssen 4 unterschiedliche Verteilungen bestimmt werden, 2 für die Features und 2 für die Klassenlabel."
      ],
      "metadata": {
        "id": "xmi9-ZwAozWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Wir importieren weitere notwendige Bibliotheken\n",
        "from sklearn.datasets import make_blobs\n",
        "from scipy.stats import norm\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "\n",
        "# Erstellen der Verteilungen für die Datenbeispiele\n",
        "def fit_distribution(data):\n",
        "\t# Schätzen der Parameter Mean und Std\n",
        "\tmu = mean(data)\n",
        "\tsigma = std(data)\n",
        "\tprint(\"Mean: \"+str(mu), \"Standardabweichung: \"+str(sigma))\n",
        "\tdist = norm(mu, sigma)\n",
        "\treturn dist\n",
        "\n",
        "# Daten nach Klassenlabel sortieren\n",
        "Xy0 = X[y == 0]\n",
        "Xy1 = X[y == 1]\n",
        "print(Xy0.shape, Xy1.shape)\n",
        "\n",
        "# Priors bestimmen ~ Anteil der Daten eines Klassenlabels an der Gesamtmenge der Daten\n",
        "priory0 = len(Xy0) / len(X)\n",
        "priory1 = len(Xy1) / len(X)\n",
        "print(priory0, priory1)\n",
        "\n",
        "\n",
        "# Parameter schätzen für Klassenlabel 0\n",
        "X1y0 = fit_distribution(Xy0[:, 0])\n",
        "X2y0 = fit_distribution(Xy0[:, 1])\n",
        "# Parameter schätzen für Klassenlabel 1\n",
        "X1y1 = fit_distribution(Xy1[:, 0])\n",
        "X2y1 = fit_distribution(Xy1[:, 1])"
      ],
      "metadata": {
        "id": "8qrurjlAKxCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Schritt 3: Wir bestimmen die Wahrscheinlichkeit beispielhafter Datenpunkte"
      ],
      "metadata": {
        "id": "3nJZlHrN7MZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bestimmung der bedingten Wahrscheinlichkeit aus X = Datenpunkt (2 Features), prior = Anteil an den Gesamtdaten, dist1|dist2 = bedingte Wahrschienlichkeitsverteilung für jede Variable\n",
        "# Kann hier mehr als Score betrachtet werden, da berechnete Wahrscheinlichkeit nicht normalisiert ist - Summe der Wahrscheinlichkeiten ergibt nicht 1\n",
        "def probability(X, prior, dist1, dist2):\n",
        "\treturn prior * dist1.pdf(X[0]) * dist2.pdf(X[1])\n",
        "\n",
        "# Klassifizierung eines Beispiels - Erster Eintrag in unserem Datensatz\n",
        "Xsample, ysample = X[0], y[0]\n",
        "py0 = probability(Xsample, priory0, X1y0, X2y0)\n",
        "py1 = probability(Xsample, priory1, X1y1, X2y1)\n",
        "print('P(y=0 | %s) = %.3f' % (Xsample, py0*100))\n",
        "print('P(y=1 | %s) = %.3f' % (Xsample, py1*100))\n",
        "print('Truth: y=%d' % ysample)"
      ],
      "metadata": {
        "id": "nkULyjMwK2k0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Beispiel Naive Bayes Classifier - Der pragmatische Weg"
      ],
      "metadata": {
        "id": "rewtOTLfKhj7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In der Praxis werden optimierte Implementierungen von Naive Bayes Algorithmen genutzt. Die scikit-learn Bibliothek bietet Implementierungen für verschiedene Wahrscheinlichkeitsverteilungen an."
      ],
      "metadata": {
        "id": "w_v884cm_SNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import der Bibliotheken\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Wir genereiren wieder einen Datensatz\n",
        "X, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n",
        "\n",
        "# Wir initialisieren das Modell (Gaußsche Verteilung)\n",
        "model = GaussianNB()\n",
        "\n",
        "# Anpassen des Modells auf unsere Daten\n",
        "model.fit(X, y)\n",
        "\n",
        "# Vorhersage auf ein Datenbeispiel\n",
        "Xsample, ysample = [X[0]], y[0]\n",
        "\n",
        "# Vorhersage \"normalisierter\" Wahrscheinlichkeiten\n",
        "yhat_prob = model.predict_proba(Xsample)\n",
        "print('Predicted Probabilities: ', yhat_prob)\n",
        "\n",
        "# Vorhersage der Klassenzugehörigkeit\n",
        "yhat_class = model.predict(Xsample)\n",
        "print('Predicted Class: ', yhat_class)\n",
        "print('Truth: y=%d' % ysample)"
      ],
      "metadata": {
        "id": "XuM708oaLCQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Anwendungsbeispiel 2**"
      ],
      "metadata": {
        "id": "Q_Qz22lhDbOy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Business Use Case:** Wir, als Golfplatzeigentümer, sind stets darauf bedacht unseren Mitgliedern einen guten Service anzubieten. In einer internen Sitzung, wurde entschieden eine App zu entwickeln, die den Mitgliedern Auskunft darüber gibt, ob die aktuelle Wetterlage ein optimales Golf-Erlebnis ermöglicht. Um die relevanten Umweltfaktoren zu messen, wurden Sensoren auf dem Golfplatz ausgebracht, die das aktuelle Wetter, die Temperatur, Feuchtigkeit und Wind messen. Grundlage der App soll ein Klassifikationsmodell sein, dass in der Lage ist auf Basis der ermittelten Umweltfaktoren eine Entscheidung für oder gegen das Golfspielen zu treffen. Je Vertrauenswürdiger das Ergebnis der App ist, desto höher ist der Mehrwert für unsere Mitglieder."
      ],
      "metadata": {
        "id": "lUfHeTWqUskV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Import aller notwendigen Bibliotheken\n",
        "import pandas as pd\n",
        "import numpy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB"
      ],
      "metadata": {
        "id": "80B-qQyXuQRJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "golf_file = \"Golf.csv\"\n",
        "\n",
        "#Einlesen der Daten in einen Pandas Dataframe\n",
        "golf_data = pd.read_csv(golf_file, sep=\",\")\n",
        "\n",
        "#Ausgabe der ersten 10 Zeilen\n",
        "golf_data.head(10)"
      ],
      "metadata": {
        "id": "52rU9tjyx1O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#String-Werte der Spalte outlook werden auf numerische Werte gemapped, um die Daten verarbeiten zu können\n",
        "d = {'sunny': 1, 'overcast': 2, 'rainy': 3}\n",
        "golf_data.outlook = [d[item] for item in golf_data.outlook.astype(str)]\n",
        "\n",
        "golf_data.head(10)"
      ],
      "metadata": {
        "id": "6ZrWiZ5DyCex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Aufteilen der Daten in Trainings- und Testdaten\n",
        "train, test = train_test_split(golf_data,test_size=0.3, random_state=0)\n",
        "\n",
        "#Initialisierung von Gaussian Naive Bayes\n",
        "naive_b = GaussianNB()\n",
        "\n",
        "#Als Trainings-Features werden alle Spalten bis auf die Spalte play genutzt\n",
        "train_features = train.iloc[:,0:4]\n",
        "#Die Spalte play nutzen wir als Label\n",
        "train_label = train.iloc[:,4]\n",
        "\n",
        "#Geleuches Vorgehen mit dem Testdatensatz\n",
        "test_features = test.iloc[:,0:4]\n",
        "test_label = test.iloc[:,4]"
      ],
      "metadata": {
        "id": "L7idPUGuyG7h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Trainieren des Modells\n",
        "naive_b.fit(train_features, train_label)"
      ],
      "metadata": {
        "id": "VcD5dkmsVWdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ausgabe der Wahrscheinlichkeiten für einzelne Instanz\n",
        "probas = naive_b.predict_proba(test_features)\n",
        "\n",
        "print(probas[0])\n",
        "print(test_label[:1])"
      ],
      "metadata": {
        "id": "rL6_uR2aVYm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Erstellen eines Dataframes, um reale und vorhergesagte Werte gegenüberzustellen\n",
        "test_data = pd.concat([test_features, test_label], axis=1)\n",
        "test_data[\"prediction\"] = naive_b.predict(test_features)\n",
        "\n",
        "print(test_data)\n",
        "\n",
        "print(\"Naive Bayes Accuracy:\", naive_b.score(test_features,test_label))\n"
      ],
      "metadata": {
        "id": "7Lt7rT6tyMWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Ressourcen zum Lernen**"
      ],
      "metadata": {
        "id": "8GRxYW2u7Re8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Probabilistic Machine Learning: An Introduction (Kevin Murphy): https://probml.github.io/pml-book/book1.html\n",
        "*   Probability for Machine Learning (Jason Brownlee): https://machinelearningmastery.com/probability-for-machine-learning/\n",
        "*   Vorlesungsreihe Probabilistic Machine Learning, Universität Tübingen: https://uni-tuebingen.de/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/methoden-des-maschinellen-lernens/lehre/probabilistic-machine-learning/\n",
        "*   Machine Learning with PyTorch and Scikit-Learn: https://www.amazon.de/Machine-Learning-PyTorch-Scikit-Learn-learning/dp/1801819319/ref=asc_df_1801819319/?tag=googshopde-21&linkCode=df0&hvadid=546595362221&hvpos=&hvnetw=g&hvrand=8259230891533996799&hvpone=&hvptwo=&hvqmt=&hvdev=c&hvdvcmdl=&hvlocint=&hvlocphy=9042982&hvtargid=pla-1763569602912&psc=1&th=1&psc=1\n",
        "* Kleinere Übungen zum Thema Wahrscheinlichkeit in Python: https://livebook.manning.com/book/data-science-bookcamp/chapter-1/\n",
        "\n"
      ],
      "metadata": {
        "id": "Pc94misI7ab2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Zusammenfassung**"
      ],
      "metadata": {
        "id": "VgKzDJ9H9XUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Wir haben einige Grundlagen zum Thema Wahrscheinlichkeit und Machine Learning wiederholt - Ihr müsst keine Mathematiker sein, um ML **anzuwenden**!\n",
        "*   Wir haben uns zwei Anwendungsbeispiele angesehen, die auf Basis eines Naive Bayes Classifiers, ein Algorithmus aus dem Bereich des Probabilistic Machine Learning, Klassifikationsprobleme lösen sollen\n",
        "*   Wie wir sehen konnten geben uns probabilistische Verfahren die Möglichkeit Unsicherheiten einer Entscheidung abzuschätzen\n",
        "*   Es existieren jede Menge Ressourcen zum Thema Probabilistic Machine Learning (eher ein fortgeschrittenes Thema)\n",
        "\n"
      ],
      "metadata": {
        "id": "JKcbOafo9dUo"
      }
    }
  ]
}